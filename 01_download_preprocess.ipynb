{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-03T20:28:47.324986Z",
     "start_time": "2025-12-03T20:28:44.614880Z"
    }
   },
   "source": [
    "import unicodedata\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import ollama\n",
    "import torch"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rober\\anaconda3\\envs\\MT_lab_project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T20:28:48.637131Z",
     "start_time": "2025-12-03T20:28:48.631834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_text(text):\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = text.replace(\" ,\", \",\").replace(\" .\", \".\")\n",
    "    return text\n",
    "\n",
    "def clean_pair(src, tgt):\n",
    "    if len(src.split()) < 2 or len(tgt.split()) < 2:\n",
    "        return None\n",
    "    if len(src.split()) > 150 or len(tgt.split()) > 150:\n",
    "        return None\n",
    "\n",
    "    ratio = len(src.split()) / max(len(tgt.split()), 1)\n",
    "    if ratio > 3.0 or ratio < (1/3):\n",
    "        return None\n",
    "\n",
    "    return src, tgt\n",
    "\n",
    "def dump_jsonl(path, data):\n",
    "    with open(path, \"w\", encoding=\"utf8\") as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def prepare_wmt16(output_root=\"data/raw\"):\n",
    "    Path(output_root).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ds = load_dataset(\"wmt16\", \"ro-en\")\n",
    "\n",
    "    for split in [\"train\", \"validation\", \"test\"]:\n",
    "        processed = []\n",
    "        seen = set()\n",
    "\n",
    "        for item in tqdm(ds[split]):\n",
    "            src = normalize_text(item[\"translation\"][\"en\"])\n",
    "            tgt = normalize_text(item[\"translation\"][\"ro\"])\n",
    "\n",
    "            cleaned = clean_pair(src, tgt)\n",
    "            if not cleaned:\n",
    "                continue\n",
    "\n",
    "            # Dedup\n",
    "            key = (src, tgt)\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "\n",
    "            processed.append({\"src\": src, \"tgt\": tgt})\n",
    "\n",
    "        dump_jsonl(f\"{output_root}/{split}.jsonl\", processed)\n",
    "        print(f\"> Saved {len(processed)} items for split={split}\")"
   ],
   "id": "ce3275b2adfcc5af",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T15:14:33.773239Z",
     "start_time": "2025-12-03T15:14:00.584045Z"
    }
   },
   "cell_type": "code",
   "source": "prepare_wmt16()",
   "id": "b572f60e8f0a1f10",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 610320/610320 [00:20<00:00, 29085.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Saved 596819 items for split=train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1999/1999 [00:00<00:00, 28667.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Saved 1996 items for split=validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1999/1999 [00:00<00:00, 28906.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Saved 1994 items for split=test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Baseline Translation Using an Open LLM",
   "id": "666fcc1f6cb43f99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T20:28:56.939592Z",
     "start_time": "2025-12-03T20:28:56.936252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"Translate the following English sentence into natural Romanian. Don't provide any notes or explanation - provide only the translation.\\n\\n\"\n",
    "    \"English: {src}\\n\\nRomanian:\"\n",
    ")"
   ],
   "id": "379cdb298b305c18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cpu\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T20:29:00.801049Z",
     "start_time": "2025-12-03T20:29:00.796015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "\n",
    "def translate(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def save_jsonl(path, items):\n",
    "    with open(path, \"w\", encoding=\"utf8\") as f:\n",
    "        for item in items:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def translate_text(model_name, src):\n",
    "    prompt = PROMPT_TEMPLATE.format(src=src)\n",
    "    response = ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        options={\"num_predict\": 200}\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "def batch_translate_ollama(input_path, output_path, model_name=\"llama3:latest\"):\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    results = []\n",
    "\n",
    "    for item in tqdm(load_jsonl(input_path)):\n",
    "        mt = translate_text(model_name, item[\"src\"])\n",
    "        results.append({\n",
    "            \"src\": item[\"src\"],\n",
    "            \"ref\": item[\"tgt\"],\n",
    "            \"mt\": mt\n",
    "        })\n",
    "\n",
    "    save_jsonl(output_path, results)\n",
    "    print(f\"> saved {len(results)} translations to {output_path}\")"
   ],
   "id": "2f88a68a09cc6a28",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T17:48:08.186131Z",
     "start_time": "2025-12-03T17:27:18.081251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_translate_ollama(\n",
    "    \"data/raw/test.jsonl\",\n",
    "    \"experiments/baseline/translations.jsonl\",\n",
    "    model_name=\"llama3:latest\"\n",
    ")"
   ],
   "id": "50056f5895b4955f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1994it [20:50,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> saved 1994 translations to experiments/baseline/translations.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T20:29:58.841642Z",
     "start_time": "2025-12-03T20:29:55.805169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import math\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class MGPTPerplexityScorer:\n",
    "    def __init__(self, model_name=\"ai-forever/mGPT\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n",
    "\n",
    "    def sentence_ppl(self, text):\n",
    "        enc = self.tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            loss = self.model(**enc, labels=enc[\"input_ids\"]).loss\n",
    "        return math.exp(loss.item())\n",
    "\n",
    "def compute_tsr_spans(sentence):\n",
    "    \"\"\"\n",
    "    Placeholder\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    spans = []\n",
    "    literal_count = 0\n",
    "\n",
    "    # naive heuristic: English cognates or obvious calques\n",
    "    cognates = {\"informativ\", \"accident\", \"activitate\", \"popular\", \"important\"}\n",
    "\n",
    "    for i, w in enumerate(words):\n",
    "        if w.lower() in cognates:\n",
    "            literal_count += 1\n",
    "            spans.append([i, i+1])\n",
    "\n",
    "    tsr = literal_count / max(len(words), 1)\n",
    "    return spans, tsr"
   ],
   "id": "904152bd85489438",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T20:30:03.482951Z",
     "start_time": "2025-12-03T20:30:03.478760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "\n",
    "def run_analysis(in_path, out_path):\n",
    "    scorer = MGPTPerplexityScorer()\n",
    "    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf8\") as fout:\n",
    "        for item in tqdm(load_jsonl(in_path)):\n",
    "            sentence = item[\"mt\"]\n",
    "            ppl = scorer.sentence_ppl(sentence)\n",
    "            spans, tsr = compute_tsr_spans(sentence)\n",
    "\n",
    "            rec = {\n",
    "                \"src\": item[\"src\"],\n",
    "                \"ref\": item[\"ref\"],\n",
    "                \"mt\": sentence,\n",
    "                \"ppl\": ppl,\n",
    "                \"tsr\": tsr,\n",
    "                \"spans\": spans\n",
    "            }\n",
    "            fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")"
   ],
   "id": "9c808100590dc1db",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T19:59:01.420030Z",
     "start_time": "2025-12-03T19:39:24.523100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "run_analysis(\n",
    "    \"experiments/baseline/translations.jsonl\",\n",
    "    \"experiments/baseline/translationese.jsonl\"\n",
    ")"
   ],
   "id": "4e8e4ca42bab61fe",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rober\\anaconda3\\envs\\MT_lab_project\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rober\\.cache\\huggingface\\hub\\models--ai-forever--mGPT. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "0it [00:00, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
      "1994it [14:38,  2.27it/s]\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
